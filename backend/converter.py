"""
Web page converter - scrapes URLs and generates PDF/EPUB.
"""

import base64
import hashlib
import os
import re
import tempfile
import time
from datetime import datetime
from urllib.parse import urlparse, urljoin

import httpx
from bs4 import BeautifulSoup
from ebooklib import epub
from playwright.async_api import async_playwright


class WebConverter:
    """Converts web pages to PDF and EPUB formats."""

    def __init__(self):
        self.temp_dir = tempfile.gettempdir()

    async def convert(self, url: str) -> dict:
        """
        Convert a URL to both PDF and EPUB.
        Returns dict with pdf_path, epub_path, and title.
        """
        # Scrape the page
        html, title, pdf_bytes = await self._scrape_page(url)

        # Parse content
        content = self._extract_content(html, url)

        # Generate safe filename
        safe_title = self._safe_filename(title)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = f"{safe_title}_{timestamp}"

        # Save PDF (generated by Playwright)
        pdf_path = os.path.join(self.temp_dir, f"{base_name}.pdf")
        with open(pdf_path, "wb") as f:
            f.write(pdf_bytes)

        # Generate EPUB
        epub_path = os.path.join(self.temp_dir, f"{base_name}.epub")
        self._generate_epub(title, content, epub_path, url)

        return {
            "pdf_path": pdf_path,
            "epub_path": epub_path,
            "title": safe_title
        }

    def _calculate_page_size(self, viewport_width: int, viewport_height: int) -> dict:
        """
        Calculate PDF page size from CSS viewport dimensions.

        Uses a standard conversion: 1 CSS pixel â‰ˆ 0.264mm (96 DPI standard)
        This ensures the PDF looks the same as on the user's device.

        Returns dict with width and height in mm.
        """
        # Standard CSS pixel to mm conversion (96 DPI = 0.264mm per pixel)
        px_to_mm = 0.264

        # Ensure portrait orientation
        if viewport_width > viewport_height:
            viewport_width, viewport_height = viewport_height, viewport_width

        # Handle unusual aspect ratios (e.g., foldable phones in square-ish mode)
        # If ratio < 1.3, use standard phone aspect ratio (9:19.5) for better readability
        aspect_ratio = viewport_height / viewport_width
        if aspect_ratio < 1.3:
            aspect_ratio = 19.5 / 9  # ~2.17, standard modern phone ratio
            viewport_height = int(viewport_width * aspect_ratio)

        # Calculate physical dimensions
        width_mm = viewport_width * px_to_mm
        height_mm = viewport_height * px_to_mm

        # Cap height to avoid extremely long pages
        max_height_mm = 250
        if height_mm > max_height_mm:
            height_mm = max_height_mm

        return {
            "width": f"{width_mm:.0f}mm",
            "height": f"{height_mm:.0f}mm"
        }

    async def convert_with_progress(self, url: str, viewport_width: int = 430, viewport_height: int = 932):
        """
        Convert a URL to both PDF and EPUB with progress updates.
        Yields progress dict: {"progress": 0-100, "message": "status"}
        Final yield includes the result data.

        Args:
            url: The webpage URL to convert
            viewport_width: User's CSS viewport width (default: iPhone 16 Pro Max)
            viewport_height: User's CSS viewport height (default: iPhone 16 Pro Max)
        """
        start_time = time.time()

        # Ensure portrait orientation for viewport
        if viewport_width > viewport_height:
            viewport_width, viewport_height = viewport_height, viewport_width

        # Handle unusual aspect ratios (e.g., foldable phones in square-ish mode)
        aspect_ratio = viewport_height / viewport_width
        if aspect_ratio < 1.3:
            aspect_ratio = 19.5 / 9  # Standard phone ratio
            viewport_height = int(viewport_width * aspect_ratio)

        # Calculate PDF page size from viewport
        page_size = self._calculate_page_size(viewport_width, viewport_height)

        yield {"progress": 5, "message": "Starting conversion..."}

        async with async_playwright() as p:
            browser = await p.chromium.launch()

            # Use custom viewport matching user's device, with mobile user agent
            context = await browser.new_context(
                viewport={"width": viewport_width, "height": viewport_height},
                user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1",
                locale="zh-CN",
                device_scale_factor=3,
                is_mobile=True,
                has_touch=True,
            )
            page = await context.new_page()

            yield {"progress": 10, "message": "Loading page..."}

            # Use domcontentloaded instead of networkidle (faster, more reliable)
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)

            yield {"progress": 25, "message": "Page loaded, rendering content..."}

            # Wait for initial content to render
            await page.wait_for_timeout(2000)

            yield {"progress": 35, "message": "Scrolling to load images..."}

            # Scroll down to trigger lazy-loading of images
            await page.evaluate("""
                async () => {
                    const delay = ms => new Promise(resolve => setTimeout(resolve, ms));
                    for (let i = 0; i < document.body.scrollHeight; i += 300) {
                        window.scrollTo(0, i);
                        await delay(100);
                    }
                    window.scrollTo(0, 0);
                }
            """)

            yield {"progress": 45, "message": "Waiting for images to load..."}

            # Wait for images to load
            await page.wait_for_timeout(3000)

            # Try to wait for all images to be loaded
            await page.evaluate("""
                () => {
                    const images = document.querySelectorAll('img');
                    return Promise.all(Array.from(images).map(img => {
                        if (img.complete) return Promise.resolve();
                        return new Promise(resolve => {
                            img.onload = resolve;
                            img.onerror = resolve;
                            setTimeout(resolve, 5000);
                        });
                    }));
                }
            """)

            yield {"progress": 55, "message": "Preparing page for PDF..."}

            # Get page title
            title = await page.title()
            if not title:
                title = "Untitled"

            # Get HTML content
            html = await page.content()

            # Use JavaScript to modify DOM for PDF output
            await page.evaluate("""
                () => {
                    // Inject Noto fonts for consistent Chinese rendering
                    const style = document.createElement('style');
                    style.textContent = `
                        * {
                            font-family: "Noto Serif SC", "Noto Sans SC", serif !important;
                        }
                    `;
                    document.head.appendChild(style);

                    // Make images fit the page width
                    const images = document.querySelectorAll('img');
                    images.forEach(img => {
                        img.style.maxWidth = '100%';
                        img.style.height = 'auto';
                    });

                    // Hide unnecessary UI elements
                    const hideSelectors = [
                        '.wx-root', '#js_pc_qr_code', '.qr_code_pc',
                        '.rich_media_meta_list', '#js_tags_preview_toast',
                        '.rich_media_tool', '.weui-desktop-popover'
                    ];
                    hideSelectors.forEach(selector => {
                        document.querySelectorAll(selector).forEach(el => {
                            el.style.display = 'none';
                        });
                    });
                }
            """)

            yield {"progress": 65, "message": "Generating PDF..."}

            # Generate PDF with dynamic page size based on user's screen
            pdf_bytes = await page.pdf(
                width=page_size["width"],
                height=page_size["height"],
                print_background=True,
                margin={"top": "5mm", "bottom": "5mm", "left": "5mm", "right": "5mm"}
            )

            await browser.close()

        yield {"progress": 75, "message": "Processing content..."}

        # Parse content
        content = self._extract_content(html, url)

        # Generate safe filename
        safe_title = self._safe_filename(title)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = f"{safe_title}_{timestamp}"

        # Save PDF
        pdf_path = os.path.join(self.temp_dir, f"{base_name}.pdf")
        with open(pdf_path, "wb") as f:
            f.write(pdf_bytes)

        yield {"progress": 85, "message": "Generating EPUB..."}

        # Generate EPUB
        epub_path = os.path.join(self.temp_dir, f"{base_name}.epub")
        self._generate_epub(title, content, epub_path, url)

        # Calculate conversion time
        conversion_time = time.time() - start_time

        yield {"progress": 100, "message": "Complete!", "result": {
            "pdf_path": pdf_path,
            "epub_path": epub_path,
            "title": safe_title,
            "source_url": url,
            "viewport_dimensions": f"{viewport_width}x{viewport_height}",
            "page_size": f"{page_size['width']} x {page_size['height']}",
            "conversion_time": round(conversion_time, 1)
        }}

    async def _scrape_page(self, url: str) -> tuple[str, str, bytes]:
        """
        Scrape page using Playwright.
        Returns (html, title, pdf_bytes).
        """
        async with async_playwright() as p:
            # Use iPhone device emulation (WeChat blocks desktop browsers)
            iphone = p.devices["iPhone 14 Pro"]

            browser = await p.chromium.launch()
            context = await browser.new_context(
                **iphone,
                locale="zh-CN"
            )
            page = await context.new_page()

            # Use domcontentloaded instead of networkidle (faster, more reliable)
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)

            # Wait for initial content to render
            await page.wait_for_timeout(2000)

            # Scroll down to trigger lazy-loading of images
            await page.evaluate("""
                async () => {
                    const delay = ms => new Promise(resolve => setTimeout(resolve, ms));
                    for (let i = 0; i < document.body.scrollHeight; i += 300) {
                        window.scrollTo(0, i);
                        await delay(100);
                    }
                    window.scrollTo(0, 0);
                }
            """)

            # Wait for images to load
            await page.wait_for_timeout(3000)

            # Try to wait for all images to be loaded
            await page.evaluate("""
                () => {
                    const images = document.querySelectorAll('img');
                    return Promise.all(Array.from(images).map(img => {
                        if (img.complete) return Promise.resolve();
                        return new Promise(resolve => {
                            img.onload = resolve;
                            img.onerror = resolve;
                            setTimeout(resolve, 5000);
                        });
                    }));
                }
            """)

            # Get page title
            title = await page.title()
            if not title:
                title = "Untitled"

            # Get HTML content
            html = await page.content()

            # Use JavaScript to modify DOM for PDF output
            # Keep WeChat's native font styling, only adjust images and hide UI elements
            await page.evaluate("""
                () => {
                    // Make images fit the page width
                    const images = document.querySelectorAll('img');
                    images.forEach(img => {
                        img.style.maxWidth = '100%';
                        img.style.height = 'auto';
                    });

                    // Hide unnecessary UI elements
                    const hideSelectors = [
                        '.wx-root', '#js_pc_qr_code', '.qr_code_pc',
                        '.rich_media_meta_list', '#js_tags_preview_toast',
                        '.rich_media_tool', '.weui-desktop-popover'
                    ];
                    hideSelectors.forEach(selector => {
                        document.querySelectorAll(selector).forEach(el => {
                            el.style.display = 'none';
                        });
                    });
                }
            """)

            # Generate PDF directly from Playwright (better quality)
            pdf_bytes = await page.pdf(
                format="A4",
                print_background=True,
                margin={"top": "1cm", "bottom": "1cm", "left": "1cm", "right": "1cm"}
            )

            await browser.close()

        return html, title, pdf_bytes

    def _extract_content(self, html: str, url: str) -> dict:
        """Extract article content from HTML."""
        soup = BeautifulSoup(html, "lxml")

        # Remove scripts, styles, and other non-content elements
        for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
            tag.decompose()

        # Try to find article content (WeChat-specific selectors first)
        content_selectors = [
            {"id": "js_content"},  # WeChat article content
            {"id": "js_article"},  # WeChat article wrapper
            {"class_": "rich_media_content"},  # WeChat
            {"tag": "article"},  # Standard article tag
            {"class_": "article-content"},
            {"class_": "post-content"},
            {"class_": "entry-content"},
        ]

        content_html = None
        for selector in content_selectors:
            if "tag" in selector:
                element = soup.find(selector["tag"])
            else:
                element = soup.find(**selector)
            if element:
                content_html = str(element)
                break

        # Fallback to body if no specific content found
        if not content_html:
            body = soup.find("body")
            content_html = str(body) if body else str(soup)

        # Extract metadata
        author = None
        author_elem = soup.find(id="js_name") or soup.find(class_="author")
        if author_elem:
            author = author_elem.get_text(strip=True)

        date = None
        date_elem = soup.find(id="publish_time") or soup.find(class_="date")
        if date_elem:
            date = date_elem.get_text(strip=True)

        return {
            "html": content_html,
            "author": author or "Unknown",
            "date": date or datetime.now().strftime("%Y-%m-%d"),
            "source_url": url
        }

    def _generate_epub(self, title: str, content: dict, output_path: str, url: str):
        """Generate EPUB file from extracted content."""
        book = epub.EpubBook()

        # Set metadata
        book.set_identifier(f"readly-{hash(url)}")
        book.set_title(title)
        book.set_language("zh")
        book.add_author(content["author"])

        # Add metadata
        book.add_metadata("DC", "source", url)
        book.add_metadata("DC", "date", content["date"])

        # Process images - download and embed them
        content_html, image_items = self._process_images_for_epub(content["html"], url)

        # Add all image items to the book
        for img_item in image_items:
            book.add_item(img_item)

        # Create chapter with article content
        chapter = epub.EpubHtml(
            title=title,
            file_name="article.xhtml",
            lang="zh"
        )

        # Wrap content in proper HTML structure
        chapter_content = f"""
        <html xmlns="http://www.w3.org/1999/xhtml">
        <head>
            <title>{title}</title>
            <style>
                body {{
                    font-family: "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", sans-serif;
                    line-height: 1.8;
                    padding: 1em;
                    max-width: 800px;
                    margin: 0 auto;
                }}
                img {{
                    max-width: 100%;
                    height: auto;
                }}
                p {{
                    margin: 1em 0;
                }}
            </style>
        </head>
        <body>
            <h1>{title}</h1>
            <p><small>Author: {content['author']} | Date: {content['date']}</small></p>
            <hr/>
            {content_html}
            <hr/>
            <p><small>Source: {url}</small></p>
        </body>
        </html>
        """
        chapter.content = chapter_content

        book.add_item(chapter)

        # Add default NCX and Nav
        book.add_item(epub.EpubNcx())
        book.add_item(epub.EpubNav())

        # Define spine
        book.spine = ["nav", chapter]

        # Write EPUB file
        epub.write_epub(output_path, book)

    def _process_images_for_epub(self, html: str, base_url: str) -> tuple[str, list]:
        """Download images and prepare them for EPUB embedding."""
        soup = BeautifulSoup(html, "lxml")
        image_items = []

        for idx, img in enumerate(soup.find_all("img")):
            # Get image URL (WeChat uses data-src for lazy loading)
            img_url = img.get("data-src") or img.get("src")
            if not img_url:
                continue

            # Skip data URLs (already embedded)
            if img_url.startswith("data:"):
                continue

            # Make absolute URL
            if not img_url.startswith(("http://", "https://")):
                img_url = urljoin(base_url, img_url)

            try:
                # Download image
                img_data = self._download_image(img_url)
                if not img_data:
                    continue

                # Determine image type
                content_type = "image/jpeg"
                ext = "jpg"
                if img_url.lower().endswith(".png") or b"\x89PNG" in img_data[:10]:
                    content_type = "image/png"
                    ext = "png"
                elif img_url.lower().endswith(".gif") or b"GIF" in img_data[:10]:
                    content_type = "image/gif"
                    ext = "gif"
                elif img_url.lower().endswith(".webp") or b"WEBP" in img_data[:20]:
                    content_type = "image/webp"
                    ext = "webp"

                # Create unique filename
                img_hash = hashlib.md5(img_url.encode()).hexdigest()[:8]
                filename = f"images/img_{idx}_{img_hash}.{ext}"

                # Create EPUB image item
                img_item = epub.EpubItem(
                    uid=f"img_{idx}",
                    file_name=filename,
                    media_type=content_type,
                    content=img_data
                )
                image_items.append(img_item)

                # Update img src in HTML
                img["src"] = filename
                # Remove data-src to avoid confusion
                if img.get("data-src"):
                    del img["data-src"]

            except Exception as e:
                print(f"Failed to download image {img_url}: {e}")
                continue

        return str(soup), image_items

    def _download_image(self, url: str) -> bytes | None:
        """Download image from URL."""
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) "
                              "AppleWebKit/605.1.15 Safari/604.1",
                "Referer": "https://mp.weixin.qq.com/"
            }
            with httpx.Client(timeout=30, follow_redirects=True) as client:
                response = client.get(url, headers=headers)
                if response.status_code == 200:
                    return response.content
        except Exception as e:
            print(f"Error downloading {url}: {e}")
        return None

    def _safe_filename(self, title: str) -> str:
        """Convert title to safe filename."""
        # Remove or replace unsafe characters
        safe = re.sub(r'[<>:"/\\|?*]', "", title)
        safe = safe.strip()
        # Limit length
        if len(safe) > 100:
            safe = safe[:100]
        return safe or "article"
