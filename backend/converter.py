"""
Web page converter - scrapes URLs and generates PDF/EPUB.
"""

import os
import re
import tempfile
from datetime import datetime
from urllib.parse import urlparse

from bs4 import BeautifulSoup
from ebooklib import epub
from playwright.async_api import async_playwright


class WebConverter:
    """Converts web pages to PDF and EPUB formats."""

    def __init__(self):
        self.temp_dir = tempfile.gettempdir()

    async def convert(self, url: str) -> dict:
        """
        Convert a URL to both PDF and EPUB.
        Returns dict with pdf_path, epub_path, and title.
        """
        # Scrape the page
        html, title, pdf_bytes = await self._scrape_page(url)

        # Parse content
        content = self._extract_content(html, url)

        # Generate safe filename
        safe_title = self._safe_filename(title)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = f"{safe_title}_{timestamp}"

        # Save PDF (generated by Playwright)
        pdf_path = os.path.join(self.temp_dir, f"{base_name}.pdf")
        with open(pdf_path, "wb") as f:
            f.write(pdf_bytes)

        # Generate EPUB
        epub_path = os.path.join(self.temp_dir, f"{base_name}.epub")
        self._generate_epub(title, content, epub_path, url)

        return {
            "pdf_path": pdf_path,
            "epub_path": epub_path,
            "title": safe_title
        }

    async def _scrape_page(self, url: str) -> tuple[str, str, bytes]:
        """
        Scrape page using Playwright.
        Returns (html, title, pdf_bytes).
        """
        async with async_playwright() as p:
            # Use iPhone device emulation (WeChat blocks desktop browsers)
            iphone = p.devices["iPhone 14 Pro"]

            browser = await p.chromium.launch()
            context = await browser.new_context(
                **iphone,
                locale="zh-CN"
            )
            page = await context.new_page()

            # Use domcontentloaded instead of networkidle (faster, more reliable)
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)

            # Wait for content to render
            await page.wait_for_timeout(3000)

            # Get page title
            title = await page.title()
            if not title:
                title = "Untitled"

            # Get HTML content
            html = await page.content()

            # Generate PDF directly from Playwright (better quality)
            pdf_bytes = await page.pdf(
                format="A4",
                print_background=True,
                margin={"top": "1cm", "bottom": "1cm", "left": "1cm", "right": "1cm"}
            )

            await browser.close()

        return html, title, pdf_bytes

    def _extract_content(self, html: str, url: str) -> dict:
        """Extract article content from HTML."""
        soup = BeautifulSoup(html, "lxml")

        # Remove scripts, styles, and other non-content elements
        for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
            tag.decompose()

        # Try to find article content (WeChat-specific selectors first)
        content_selectors = [
            {"id": "js_content"},  # WeChat article content
            {"id": "js_article"},  # WeChat article wrapper
            {"class_": "rich_media_content"},  # WeChat
            {"tag": "article"},  # Standard article tag
            {"class_": "article-content"},
            {"class_": "post-content"},
            {"class_": "entry-content"},
        ]

        content_html = None
        for selector in content_selectors:
            if "tag" in selector:
                element = soup.find(selector["tag"])
            else:
                element = soup.find(**selector)
            if element:
                content_html = str(element)
                break

        # Fallback to body if no specific content found
        if not content_html:
            body = soup.find("body")
            content_html = str(body) if body else str(soup)

        # Extract metadata
        author = None
        author_elem = soup.find(id="js_name") or soup.find(class_="author")
        if author_elem:
            author = author_elem.get_text(strip=True)

        date = None
        date_elem = soup.find(id="publish_time") or soup.find(class_="date")
        if date_elem:
            date = date_elem.get_text(strip=True)

        return {
            "html": content_html,
            "author": author or "Unknown",
            "date": date or datetime.now().strftime("%Y-%m-%d"),
            "source_url": url
        }

    def _generate_epub(self, title: str, content: dict, output_path: str, url: str):
        """Generate EPUB file from extracted content."""
        book = epub.EpubBook()

        # Set metadata
        book.set_identifier(f"readly-{hash(url)}")
        book.set_title(title)
        book.set_language("zh")
        book.add_author(content["author"])

        # Add metadata
        book.add_metadata("DC", "source", url)
        book.add_metadata("DC", "date", content["date"])

        # Create chapter with article content
        chapter = epub.EpubHtml(
            title=title,
            file_name="article.xhtml",
            lang="zh"
        )

        # Wrap content in proper HTML structure
        chapter_content = f"""
        <html xmlns="http://www.w3.org/1999/xhtml">
        <head>
            <title>{title}</title>
            <style>
                body {{
                    font-family: "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", sans-serif;
                    line-height: 1.8;
                    padding: 1em;
                    max-width: 800px;
                    margin: 0 auto;
                }}
                img {{
                    max-width: 100%;
                    height: auto;
                }}
                p {{
                    margin: 1em 0;
                }}
            </style>
        </head>
        <body>
            <h1>{title}</h1>
            <p><small>Author: {content['author']} | Date: {content['date']}</small></p>
            <hr/>
            {content['html']}
            <hr/>
            <p><small>Source: {url}</small></p>
        </body>
        </html>
        """
        chapter.content = chapter_content

        book.add_item(chapter)

        # Add default NCX and Nav
        book.add_item(epub.EpubNcx())
        book.add_item(epub.EpubNav())

        # Define spine
        book.spine = ["nav", chapter]

        # Write EPUB file
        epub.write_epub(output_path, book)

    def _safe_filename(self, title: str) -> str:
        """Convert title to safe filename."""
        # Remove or replace unsafe characters
        safe = re.sub(r'[<>:"/\\|?*]', "", title)
        safe = safe.strip()
        # Limit length
        if len(safe) > 100:
            safe = safe[:100]
        return safe or "article"
