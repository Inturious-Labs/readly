"""
Web page converter - scrapes URLs and generates PDF/EPUB.
Supports WeChat articles natively and any generic website.
"""

import base64
import hashlib
import os
import re
import tempfile
import time
from datetime import datetime
from urllib.parse import urlparse, urljoin

import httpx
from bs4 import BeautifulSoup
from ebooklib import epub
from playwright.async_api import async_playwright


# Persistent output directory for converted files
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), "output")


def _is_wechat_url(url: str) -> bool:
    """Check if a URL is a WeChat article."""
    hostname = urlparse(url).hostname or ""
    return "weixin.qq.com" in hostname or "wechat.com" in hostname


class WebConverter:
    """Converts web pages to PDF and EPUB formats."""

    def __init__(self):
        self.output_dir = OUTPUT_DIR
        os.makedirs(self.output_dir, exist_ok=True)

    async def convert(self, url: str) -> dict:
        """
        Convert a URL to both PDF and EPUB.
        Returns dict with pdf_path, epub_path, and title.
        """
        # Scrape the page
        html, title, pdf_bytes = await self._scrape_page(url)

        # Parse content
        content = self._extract_content(html, url)

        # Generate safe filename
        safe_title = self._safe_filename(title)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = f"{safe_title}_{timestamp}"

        # Save PDF (generated by Playwright)
        pdf_path = os.path.join(self.output_dir, f"{base_name}.pdf")
        with open(pdf_path, "wb") as f:
            f.write(pdf_bytes)

        # Generate EPUB
        epub_path = os.path.join(self.output_dir, f"{base_name}.epub")
        self._generate_epub(title, content, epub_path, url)

        return {
            "pdf_path": pdf_path,
            "epub_path": epub_path,
            "title": safe_title
        }

    def _calculate_page_size(self, viewport_width: int, viewport_height: int) -> dict:
        """
        Calculate PDF page size from CSS viewport dimensions.

        Uses a standard conversion: 1 CSS pixel ≈ 0.264mm (96 DPI standard)
        This ensures the PDF looks the same as on the user's device.

        Returns dict with width and height in mm.
        """
        # Standard CSS pixel to mm conversion (96 DPI = 0.264mm per pixel)
        px_to_mm = 0.264

        # Ensure portrait orientation
        if viewport_width > viewport_height:
            viewport_width, viewport_height = viewport_height, viewport_width

        # Handle unusual aspect ratios (e.g., foldable phones in square-ish mode)
        # If ratio < 1.3, use standard phone aspect ratio (9:19.5) for better readability
        aspect_ratio = viewport_height / viewport_width
        if aspect_ratio < 1.3:
            aspect_ratio = 19.5 / 9  # ~2.17, standard modern phone ratio
            viewport_height = int(viewport_width * aspect_ratio)

        # Calculate physical dimensions
        width_mm = viewport_width * px_to_mm
        height_mm = viewport_height * px_to_mm

        # Cap height to avoid extremely long pages
        max_height_mm = 250
        if height_mm > max_height_mm:
            height_mm = max_height_mm

        return {
            "width": f"{width_mm:.0f}mm",
            "height": f"{height_mm:.0f}mm"
        }

    async def convert_with_progress(self, url: str, viewport_width: int = 430, viewport_height: int = 932):
        """
        Convert a URL to both PDF and EPUB with progress updates.
        Yields progress dict: {"progress": 0-100, "message": "status"}
        Final yield includes the result data.

        Adapts rendering strategy based on URL type:
        - WeChat: mobile emulation, Chinese fonts, viewport-sized PDF
        - Generic: desktop rendering, site's own fonts, A4 PDF

        Args:
            url: The webpage URL to convert
            viewport_width: User's CSS viewport width (default: iPhone 16 Pro Max)
            viewport_height: User's CSS viewport height (default: iPhone 16 Pro Max)
        """
        start_time = time.time()
        is_wechat = _is_wechat_url(url)

        if is_wechat:
            # WeChat: use mobile viewport from user's device
            if viewport_width > viewport_height:
                viewport_width, viewport_height = viewport_height, viewport_width
            aspect_ratio = viewport_height / viewport_width
            if aspect_ratio < 1.3:
                aspect_ratio = 19.5 / 9
                viewport_height = int(viewport_width * aspect_ratio)
            page_size = self._calculate_page_size(viewport_width, viewport_height)
        else:
            # Generic: A4 page, desktop viewport
            page_size = {"width": "210mm", "height": "297mm"}
            viewport_width = 1280
            viewport_height = 800

        yield {"progress": 5, "message": "Starting conversion..."}

        async with async_playwright() as p:
            browser = await p.chromium.launch()

            if is_wechat:
                # WeChat: mobile emulation (WeChat blocks desktop browsers)
                context = await browser.new_context(
                    viewport={"width": viewport_width, "height": viewport_height},
                    user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1",
                    locale="zh-CN",
                    device_scale_factor=3,
                    is_mobile=True,
                    has_touch=True,
                )
            else:
                # Generic: desktop browser
                context = await browser.new_context(
                    viewport={"width": viewport_width, "height": viewport_height},
                    user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
                    locale="en-US",
                    device_scale_factor=1,
                )

            page = await context.new_page()

            yield {"progress": 10, "message": "Loading page..."}

            # Use domcontentloaded instead of networkidle (faster, more reliable)
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)

            yield {"progress": 25, "message": "Page loaded, rendering content..."}

            # Wait for initial content to render
            await page.wait_for_timeout(2000)

            yield {"progress": 35, "message": "Scrolling to load images..."}

            # Scroll down to trigger lazy-loading of images
            await page.evaluate("""
                async () => {
                    const delay = ms => new Promise(resolve => setTimeout(resolve, ms));
                    for (let i = 0; i < document.body.scrollHeight; i += 300) {
                        window.scrollTo(0, i);
                        await delay(100);
                    }
                    window.scrollTo(0, 0);
                }
            """)

            yield {"progress": 45, "message": "Waiting for images to load..."}

            # Wait for images to load
            await page.wait_for_timeout(3000)

            # Try to wait for all images to be loaded
            await page.evaluate("""
                () => {
                    const images = document.querySelectorAll('img');
                    return Promise.all(Array.from(images).map(img => {
                        if (img.complete) return Promise.resolve();
                        return new Promise(resolve => {
                            img.onload = resolve;
                            img.onerror = resolve;
                            setTimeout(resolve, 5000);
                        });
                    }));
                }
            """)

            yield {"progress": 55, "message": "Preparing page for PDF..."}

            # Get page title
            title = await page.title()
            if not title:
                title = "Untitled"

            # Get HTML content (before DOM modifications, for EPUB extraction)
            html = await page.content()

            if is_wechat:
                # WeChat: inject Chinese fonts and hide WeChat UI chrome
                await page.evaluate("""
                    () => {
                        const style = document.createElement('style');
                        style.textContent = `
                            * {
                                font-family: "Noto Serif SC", "Noto Serif", "Noto Sans SC", serif !important;
                            }
                        `;
                        document.head.appendChild(style);

                        const images = document.querySelectorAll('img');
                        images.forEach(img => {
                            img.style.maxWidth = '100%';
                            img.style.height = 'auto';
                        });

                        const hideSelectors = [
                            '.wx-root', '#js_pc_qr_code', '.qr_code_pc',
                            '.rich_media_meta_list', '#js_tags_preview_toast',
                            '.rich_media_tool', '.weui-desktop-popover'
                        ];
                        hideSelectors.forEach(selector => {
                            document.querySelectorAll(selector).forEach(el => {
                                el.style.display = 'none';
                            });
                        });
                    }
                """)
            else:
                # Generic: hide site chrome, ensure images fit, improve print layout
                await page.evaluate("""
                    () => {
                        const images = document.querySelectorAll('img');
                        images.forEach(img => {
                            img.style.maxWidth = '100%';
                            img.style.height = 'auto';
                        });

                        const hideSelectors = [
                            // Generic site chrome
                            'header', 'nav', '.navbar', '.navigation', '.nav-bar', '.site-nav',
                            '.site-header', '.header', '#header',
                            '.sidebar', '.widget-area', '#sidebar',
                            'footer', '.site-footer', '#footer',
                            '.comments', '#comments', '.comment-section',
                            '.share-buttons', '.social-share', '.sharing',
                            '.cookie-notice', '.cookie-banner',
                            '.popup', '.modal', '.overlay',
                            '.ad', '.ads', '.advertisement',
                            '.related-posts', '.recommended',
                            // Substack
                            '.subscribe-widget', '.subscription-widget-wrap',
                            '.footer-wrap', '.post-ufi', '.paywall-title',
                            '.pencraft.pc-display-none',
                            // Medium
                            '.metabar', '.postActions', '.js-postShareWidget',
                            '[data-testid="headerSocialShare"]',
                            '.pw-multi-vote-icon', '.speechify-ignore',
                            '[aria-label="Member-only story"]'
                        ];
                        hideSelectors.forEach(selector => {
                            document.querySelectorAll(selector).forEach(el => {
                                el.style.display = 'none';
                            });
                        });

                        // Improve print readability
                        const style = document.createElement('style');
                        style.textContent = `
                            @media print {
                                body { max-width: 100%; }
                                img { max-width: 100%; height: auto; }
                                pre, code { white-space: pre-wrap; word-break: break-all; }
                            }
                        `;
                        document.head.appendChild(style);
                    }
                """)

            yield {"progress": 65, "message": "Generating PDF..."}

            if is_wechat:
                # WeChat: viewport-based page size for mobile reading
                pdf_bytes = await page.pdf(
                    width=page_size["width"],
                    height=page_size["height"],
                    print_background=True,
                    margin={"top": "5mm", "bottom": "5mm", "left": "5mm", "right": "5mm"}
                )
            else:
                # Generic: standard A4 with comfortable margins
                pdf_bytes = await page.pdf(
                    format="A4",
                    print_background=True,
                    margin={"top": "15mm", "bottom": "15mm", "left": "15mm", "right": "15mm"}
                )

            await browser.close()

        yield {"progress": 75, "message": "Processing content..."}

        # Parse content
        content = self._extract_content(html, url)

        # Generate safe filename
        safe_title = self._safe_filename(title)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = f"{safe_title}_{timestamp}"

        # Save PDF
        pdf_path = os.path.join(self.output_dir, f"{base_name}.pdf")
        with open(pdf_path, "wb") as f:
            f.write(pdf_bytes)

        yield {"progress": 85, "message": "Generating EPUB..."}

        # Generate EPUB
        epub_path = os.path.join(self.output_dir, f"{base_name}.epub")
        self._generate_epub(title, content, epub_path, url)

        # Calculate conversion time
        conversion_time = time.time() - start_time

        yield {"progress": 100, "message": "Complete!", "result": {
            "pdf_path": pdf_path,
            "epub_path": epub_path,
            "title": safe_title,
            "source_url": url,
            "viewport_dimensions": f"{viewport_width}x{viewport_height}",
            "page_size": f"{page_size['width']} x {page_size['height']}",
            "conversion_time": round(conversion_time, 1)
        }}

    async def _scrape_page(self, url: str) -> tuple[str, str, bytes]:
        """
        Scrape page using Playwright (legacy sync endpoint).
        Returns (html, title, pdf_bytes).
        """
        is_wechat = _is_wechat_url(url)

        async with async_playwright() as p:
            browser = await p.chromium.launch()

            if is_wechat:
                iphone = p.devices["iPhone 14 Pro"]
                context = await browser.new_context(**iphone, locale="zh-CN")
            else:
                context = await browser.new_context(
                    viewport={"width": 1280, "height": 800},
                    user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
                    locale="en-US",
                    device_scale_factor=1,
                )

            page = await context.new_page()

            await page.goto(url, wait_until="domcontentloaded", timeout=60000)
            await page.wait_for_timeout(2000)

            # Scroll to trigger lazy-loading
            await page.evaluate("""
                async () => {
                    const delay = ms => new Promise(resolve => setTimeout(resolve, ms));
                    for (let i = 0; i < document.body.scrollHeight; i += 300) {
                        window.scrollTo(0, i);
                        await delay(100);
                    }
                    window.scrollTo(0, 0);
                }
            """)

            await page.wait_for_timeout(3000)

            await page.evaluate("""
                () => {
                    const images = document.querySelectorAll('img');
                    return Promise.all(Array.from(images).map(img => {
                        if (img.complete) return Promise.resolve();
                        return new Promise(resolve => {
                            img.onload = resolve;
                            img.onerror = resolve;
                            setTimeout(resolve, 5000);
                        });
                    }));
                }
            """)

            title = await page.title()
            if not title:
                title = "Untitled"

            html = await page.content()

            # DOM modifications depend on site type
            if is_wechat:
                await page.evaluate("""
                    () => {
                        const images = document.querySelectorAll('img');
                        images.forEach(img => {
                            img.style.maxWidth = '100%';
                            img.style.height = 'auto';
                        });
                        const hideSelectors = [
                            '.wx-root', '#js_pc_qr_code', '.qr_code_pc',
                            '.rich_media_meta_list', '#js_tags_preview_toast',
                            '.rich_media_tool', '.weui-desktop-popover'
                        ];
                        hideSelectors.forEach(selector => {
                            document.querySelectorAll(selector).forEach(el => {
                                el.style.display = 'none';
                            });
                        });
                    }
                """)
            else:
                await page.evaluate("""
                    () => {
                        const images = document.querySelectorAll('img');
                        images.forEach(img => {
                            img.style.maxWidth = '100%';
                            img.style.height = 'auto';
                        });
                        const hideSelectors = [
                            'header', 'nav', '.navbar', '.navigation', '.site-header',
                            '.sidebar', '.widget-area',
                            'footer', '.site-footer',
                            '.comments', '#comments',
                            '.share-buttons', '.social-share',
                            '.cookie-notice', '.ad', '.ads',
                            '.subscribe-widget', '.subscription-widget-wrap',
                            '.footer-wrap', '.post-ufi',
                            '.metabar', '.postActions'
                        ];
                        hideSelectors.forEach(selector => {
                            document.querySelectorAll(selector).forEach(el => {
                                el.style.display = 'none';
                            });
                        });
                    }
                """)

            pdf_bytes = await page.pdf(
                format="A4",
                print_background=True,
                margin={"top": "1cm", "bottom": "1cm", "left": "1cm", "right": "1cm"}
            )

            await browser.close()

        return html, title, pdf_bytes

    def _extract_content(self, html: str, url: str) -> dict:
        """Extract article content from HTML. Adapts selectors based on URL type."""
        soup = BeautifulSoup(html, "lxml")
        is_wechat = _is_wechat_url(url)

        # Remove scripts, styles, and other non-content elements
        for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
            tag.decompose()

        if is_wechat:
            content_selectors = [
                {"id": "js_content"},
                {"id": "js_article"},
                {"class_": "rich_media_content"},
                {"tag": "article"},
            ]
        else:
            content_selectors = [
                # Platform-specific selectors
                {"class_": "available-content"},  # Substack
                # Standard semantic selectors
                {"tag": "article"},               # Medium, WordPress, most blogs
                {"tag": "main"},
                # Common CMS/framework classes
                {"class_": "post-content"},
                {"class_": "entry-content"},
                {"class_": "article-content"},
                {"class_": "content"},
                {"class_": "post"},
                {"class_": "prose"},              # Tailwind CSS
                {"class_": "markdown"},           # Hugo, GitHub
                {"class_": "single-content"},     # Hugo single pages
                {"id": "content"},
                {"id": "main-content"},
                {"attrs": {"role": "main"}},
            ]

        content_html = None
        for selector in content_selectors:
            if "tag" in selector:
                element = soup.find(selector["tag"])
            elif "attrs" in selector:
                element = soup.find(attrs=selector["attrs"])
            else:
                element = soup.find(**selector)
            if element:
                content_html = str(element)
                break

        # Fallback to body if no specific content found
        if not content_html:
            body = soup.find("body")
            content_html = str(body) if body else str(soup)

        # Extract metadata — different strategies per site type
        author = None
        date = None

        if is_wechat:
            author_elem = soup.find(id="js_name") or soup.find(class_="author")
            if author_elem:
                author = author_elem.get_text(strip=True)
            date_elem = soup.find(id="publish_time") or soup.find(class_="date")
            if date_elem:
                date = date_elem.get_text(strip=True)
        else:
            # Try meta tags first (most reliable for generic sites)
            meta_author = soup.find("meta", attrs={"name": "author"})
            if meta_author:
                author = meta_author.get("content", "").strip()
            if not author:
                author_elem = (soup.find(class_="author") or
                               soup.find(attrs={"rel": "author"}) or
                               soup.find(class_="byline"))
                if author_elem:
                    author = author_elem.get_text(strip=True)

            # Try <time> tag, meta tags, then class-based selectors
            time_elem = soup.find("time")
            if time_elem:
                date = time_elem.get("datetime") or time_elem.get_text(strip=True)
            if not date:
                meta_date = soup.find("meta", attrs={"property": "article:published_time"})
                if meta_date:
                    date = meta_date.get("content", "").strip()
            if not date:
                date_elem = (soup.find(class_="date") or
                             soup.find(class_="published") or
                             soup.find(class_="post-date"))
                if date_elem:
                    date = date_elem.get_text(strip=True)

        return {
            "html": content_html,
            "author": author or "Unknown",
            "date": date or datetime.now().strftime("%Y-%m-%d"),
            "source_url": url
        }

    def _generate_epub(self, title: str, content: dict, output_path: str, url: str):
        """Generate EPUB file from extracted content."""
        book = epub.EpubBook()
        is_wechat = _is_wechat_url(url)
        lang = "zh" if is_wechat else "en"

        # Set metadata
        book.set_identifier(f"readly-{hash(url)}")
        book.set_title(title)
        book.set_language(lang)
        book.add_author(content["author"])

        # Add metadata
        book.add_metadata("DC", "source", url)
        book.add_metadata("DC", "date", content["date"])

        # Process images - download and embed them
        content_html, image_items = self._process_images_for_epub(content["html"], url)

        # Add all image items to the book
        for img_item in image_items:
            book.add_item(img_item)

        # Create chapter with article content
        chapter = epub.EpubHtml(
            title=title,
            file_name="article.xhtml",
            lang=lang
        )

        # Use appropriate font stack based on content language
        if is_wechat:
            font_family = '"Noto Serif SC", "Noto Serif", serif'
        else:
            font_family = 'Georgia, "Times New Roman", serif'

        # Wrap content in proper HTML structure
        chapter_content = f"""
        <html xmlns="http://www.w3.org/1999/xhtml">
        <head>
            <title>{title}</title>
            <style>
                body {{
                    font-family: {font_family};
                    line-height: 1.8;
                    padding: 1em;
                    max-width: 800px;
                    margin: 0 auto;
                }}
                img {{
                    max-width: 100%;
                    height: auto;
                }}
                p {{
                    margin: 1em 0;
                }}
            </style>
        </head>
        <body>
            <h1>{title}</h1>
            <p><small>Author: {content['author']} | Date: {content['date']}</small></p>
            <hr/>
            {content_html}
            <hr/>
            <p><small>Source: {url}</small></p>
        </body>
        </html>
        """
        chapter.content = chapter_content

        book.add_item(chapter)

        # Add default NCX and Nav
        book.add_item(epub.EpubNcx())
        book.add_item(epub.EpubNav())

        # Define spine
        book.spine = ["nav", chapter]

        # Write EPUB file
        epub.write_epub(output_path, book)

    def _process_images_for_epub(self, html: str, base_url: str) -> tuple[str, list]:
        """Download images and prepare them for EPUB embedding."""
        soup = BeautifulSoup(html, "lxml")
        image_items = []
        is_wechat = _is_wechat_url(base_url)

        # Derive referer from the source URL's origin
        parsed = urlparse(base_url)
        referer = f"{parsed.scheme}://{parsed.hostname}/"

        for idx, img in enumerate(soup.find_all("img")):
            # Get image URL — WeChat uses data-src for lazy loading
            if is_wechat:
                img_url = img.get("data-src") or img.get("src")
            else:
                img_url = img.get("src") or img.get("data-src")
            if not img_url:
                continue

            # Skip data URLs (already embedded)
            if img_url.startswith("data:"):
                continue

            # Make absolute URL
            if not img_url.startswith(("http://", "https://")):
                img_url = urljoin(base_url, img_url)

            try:
                # Download image
                img_data = self._download_image(img_url, referer=referer)
                if not img_data:
                    continue

                # Determine image type
                content_type = "image/jpeg"
                ext = "jpg"
                if img_url.lower().endswith(".png") or b"\x89PNG" in img_data[:10]:
                    content_type = "image/png"
                    ext = "png"
                elif img_url.lower().endswith(".gif") or b"GIF" in img_data[:10]:
                    content_type = "image/gif"
                    ext = "gif"
                elif img_url.lower().endswith(".webp") or b"WEBP" in img_data[:20]:
                    content_type = "image/webp"
                    ext = "webp"

                # Create unique filename
                img_hash = hashlib.md5(img_url.encode()).hexdigest()[:8]
                filename = f"images/img_{idx}_{img_hash}.{ext}"

                # Create EPUB image item
                img_item = epub.EpubItem(
                    uid=f"img_{idx}",
                    file_name=filename,
                    media_type=content_type,
                    content=img_data
                )
                image_items.append(img_item)

                # Update img src in HTML
                img["src"] = filename
                # Remove data-src to avoid confusion
                if img.get("data-src"):
                    del img["data-src"]

            except Exception as e:
                print(f"Failed to download image {img_url}: {e}")
                continue

        return str(soup), image_items

    def _download_image(self, url: str, referer: str = "") -> bytes | None:
        """Download image from URL with appropriate referer."""
        try:
            # Derive referer from image URL if not provided
            if not referer:
                parsed = urlparse(url)
                referer = f"{parsed.scheme}://{parsed.hostname}/"
            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                              "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
                "Referer": referer
            }
            with httpx.Client(timeout=30, follow_redirects=True) as client:
                response = client.get(url, headers=headers)
                if response.status_code == 200:
                    return response.content
        except Exception as e:
            print(f"Error downloading {url}: {e}")
        return None

    def _safe_filename(self, title: str) -> str:
        """Convert title to safe filename."""
        # Remove or replace unsafe characters
        safe = re.sub(r'[<>:"/\\|?*]', "", title)
        safe = safe.strip()
        # Limit length
        if len(safe) > 100:
            safe = safe[:100]
        return safe or "article"
