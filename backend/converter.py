"""
Web page converter - scrapes URLs and generates PDF/EPUB.
"""

import base64
import hashlib
import os
import re
import tempfile
from datetime import datetime
from urllib.parse import urlparse, urljoin

import httpx
from bs4 import BeautifulSoup
from ebooklib import epub
from playwright.async_api import async_playwright


class WebConverter:
    """Converts web pages to PDF and EPUB formats."""

    def __init__(self):
        self.temp_dir = tempfile.gettempdir()

    async def convert(self, url: str) -> dict:
        """
        Convert a URL to both PDF and EPUB.
        Returns dict with pdf_path, epub_path, and title.
        """
        # Scrape the page
        html, title, pdf_bytes = await self._scrape_page(url)

        # Parse content
        content = self._extract_content(html, url)

        # Generate safe filename
        safe_title = self._safe_filename(title)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = f"{safe_title}_{timestamp}"

        # Save PDF (generated by Playwright)
        pdf_path = os.path.join(self.temp_dir, f"{base_name}.pdf")
        with open(pdf_path, "wb") as f:
            f.write(pdf_bytes)

        # Generate EPUB
        epub_path = os.path.join(self.temp_dir, f"{base_name}.epub")
        self._generate_epub(title, content, epub_path, url)

        return {
            "pdf_path": pdf_path,
            "epub_path": epub_path,
            "title": safe_title
        }

    async def _scrape_page(self, url: str) -> tuple[str, str, bytes]:
        """
        Scrape page using Playwright.
        Returns (html, title, pdf_bytes).
        """
        async with async_playwright() as p:
            # Use iPhone device emulation (WeChat blocks desktop browsers)
            iphone = p.devices["iPhone 14 Pro"]

            browser = await p.chromium.launch()
            context = await browser.new_context(
                **iphone,
                locale="zh-CN"
            )
            page = await context.new_page()

            # Use domcontentloaded instead of networkidle (faster, more reliable)
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)

            # Wait for initial content to render
            await page.wait_for_timeout(2000)

            # Scroll down to trigger lazy-loading of images
            await page.evaluate("""
                async () => {
                    const delay = ms => new Promise(resolve => setTimeout(resolve, ms));
                    for (let i = 0; i < document.body.scrollHeight; i += 300) {
                        window.scrollTo(0, i);
                        await delay(100);
                    }
                    window.scrollTo(0, 0);
                }
            """)

            # Wait for images to load
            await page.wait_for_timeout(3000)

            # Try to wait for all images to be loaded
            await page.evaluate("""
                () => {
                    const images = document.querySelectorAll('img');
                    return Promise.all(Array.from(images).map(img => {
                        if (img.complete) return Promise.resolve();
                        return new Promise(resolve => {
                            img.onload = resolve;
                            img.onerror = resolve;
                            setTimeout(resolve, 5000);
                        });
                    }));
                }
            """)

            # Get page title
            title = await page.title()
            if not title:
                title = "Untitled"

            # Get HTML content
            html = await page.content()

            # Use JavaScript to modify DOM for better PDF readability
            # CSS alone doesn't work because WeChat uses inline styles
            await page.evaluate("""
                () => {
                    // Increase font size on all text elements
                    const textElements = document.querySelectorAll('p, span, section, div, h1, h2, h3, h4, h5, h6');
                    textElements.forEach(el => {
                        const currentSize = parseFloat(window.getComputedStyle(el).fontSize);
                        if (currentSize && currentSize < 20) {
                            el.style.fontSize = '18px';
                            el.style.lineHeight = '1.8';
                        }
                    });

                    // Make images larger - set width to 100% of container
                    const images = document.querySelectorAll('img');
                    images.forEach(img => {
                        img.style.maxWidth = '100%';
                        img.style.width = '100%';
                        img.style.height = 'auto';
                    });

                    // Hide unnecessary UI elements
                    const hideSelectors = [
                        '.wx-root', '#js_pc_qr_code', '.qr_code_pc',
                        '.rich_media_meta_list', '#js_tags_preview_toast',
                        '.rich_media_tool', '.weui-desktop-popover'
                    ];
                    hideSelectors.forEach(selector => {
                        document.querySelectorAll(selector).forEach(el => {
                            el.style.display = 'none';
                        });
                    });

                    // Increase spacing between paragraphs
                    document.querySelectorAll('p').forEach(p => {
                        p.style.marginBottom = '1em';
                    });
                }
            """)

            # Generate PDF directly from Playwright (better quality)
            pdf_bytes = await page.pdf(
                format="A4",
                print_background=True,
                margin={"top": "1cm", "bottom": "1cm", "left": "1cm", "right": "1cm"}
            )

            await browser.close()

        return html, title, pdf_bytes

    def _extract_content(self, html: str, url: str) -> dict:
        """Extract article content from HTML."""
        soup = BeautifulSoup(html, "lxml")

        # Remove scripts, styles, and other non-content elements
        for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
            tag.decompose()

        # Try to find article content (WeChat-specific selectors first)
        content_selectors = [
            {"id": "js_content"},  # WeChat article content
            {"id": "js_article"},  # WeChat article wrapper
            {"class_": "rich_media_content"},  # WeChat
            {"tag": "article"},  # Standard article tag
            {"class_": "article-content"},
            {"class_": "post-content"},
            {"class_": "entry-content"},
        ]

        content_html = None
        for selector in content_selectors:
            if "tag" in selector:
                element = soup.find(selector["tag"])
            else:
                element = soup.find(**selector)
            if element:
                content_html = str(element)
                break

        # Fallback to body if no specific content found
        if not content_html:
            body = soup.find("body")
            content_html = str(body) if body else str(soup)

        # Extract metadata
        author = None
        author_elem = soup.find(id="js_name") or soup.find(class_="author")
        if author_elem:
            author = author_elem.get_text(strip=True)

        date = None
        date_elem = soup.find(id="publish_time") or soup.find(class_="date")
        if date_elem:
            date = date_elem.get_text(strip=True)

        return {
            "html": content_html,
            "author": author or "Unknown",
            "date": date or datetime.now().strftime("%Y-%m-%d"),
            "source_url": url
        }

    def _generate_epub(self, title: str, content: dict, output_path: str, url: str):
        """Generate EPUB file from extracted content."""
        book = epub.EpubBook()

        # Set metadata
        book.set_identifier(f"readly-{hash(url)}")
        book.set_title(title)
        book.set_language("zh")
        book.add_author(content["author"])

        # Add metadata
        book.add_metadata("DC", "source", url)
        book.add_metadata("DC", "date", content["date"])

        # Process images - download and embed them
        content_html, image_items = self._process_images_for_epub(content["html"], url)

        # Add all image items to the book
        for img_item in image_items:
            book.add_item(img_item)

        # Create chapter with article content
        chapter = epub.EpubHtml(
            title=title,
            file_name="article.xhtml",
            lang="zh"
        )

        # Wrap content in proper HTML structure
        chapter_content = f"""
        <html xmlns="http://www.w3.org/1999/xhtml">
        <head>
            <title>{title}</title>
            <style>
                body {{
                    font-family: "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", sans-serif;
                    line-height: 1.8;
                    padding: 1em;
                    max-width: 800px;
                    margin: 0 auto;
                }}
                img {{
                    max-width: 100%;
                    height: auto;
                }}
                p {{
                    margin: 1em 0;
                }}
            </style>
        </head>
        <body>
            <h1>{title}</h1>
            <p><small>Author: {content['author']} | Date: {content['date']}</small></p>
            <hr/>
            {content_html}
            <hr/>
            <p><small>Source: {url}</small></p>
        </body>
        </html>
        """
        chapter.content = chapter_content

        book.add_item(chapter)

        # Add default NCX and Nav
        book.add_item(epub.EpubNcx())
        book.add_item(epub.EpubNav())

        # Define spine
        book.spine = ["nav", chapter]

        # Write EPUB file
        epub.write_epub(output_path, book)

    def _process_images_for_epub(self, html: str, base_url: str) -> tuple[str, list]:
        """Download images and prepare them for EPUB embedding."""
        soup = BeautifulSoup(html, "lxml")
        image_items = []

        for idx, img in enumerate(soup.find_all("img")):
            # Get image URL (WeChat uses data-src for lazy loading)
            img_url = img.get("data-src") or img.get("src")
            if not img_url:
                continue

            # Skip data URLs (already embedded)
            if img_url.startswith("data:"):
                continue

            # Make absolute URL
            if not img_url.startswith(("http://", "https://")):
                img_url = urljoin(base_url, img_url)

            try:
                # Download image
                img_data = self._download_image(img_url)
                if not img_data:
                    continue

                # Determine image type
                content_type = "image/jpeg"
                ext = "jpg"
                if img_url.lower().endswith(".png") or b"\x89PNG" in img_data[:10]:
                    content_type = "image/png"
                    ext = "png"
                elif img_url.lower().endswith(".gif") or b"GIF" in img_data[:10]:
                    content_type = "image/gif"
                    ext = "gif"
                elif img_url.lower().endswith(".webp") or b"WEBP" in img_data[:20]:
                    content_type = "image/webp"
                    ext = "webp"

                # Create unique filename
                img_hash = hashlib.md5(img_url.encode()).hexdigest()[:8]
                filename = f"images/img_{idx}_{img_hash}.{ext}"

                # Create EPUB image item
                img_item = epub.EpubItem(
                    uid=f"img_{idx}",
                    file_name=filename,
                    media_type=content_type,
                    content=img_data
                )
                image_items.append(img_item)

                # Update img src in HTML
                img["src"] = filename
                # Remove data-src to avoid confusion
                if img.get("data-src"):
                    del img["data-src"]

            except Exception as e:
                print(f"Failed to download image {img_url}: {e}")
                continue

        return str(soup), image_items

    def _download_image(self, url: str) -> bytes | None:
        """Download image from URL."""
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) "
                              "AppleWebKit/605.1.15 Safari/604.1",
                "Referer": "https://mp.weixin.qq.com/"
            }
            with httpx.Client(timeout=30, follow_redirects=True) as client:
                response = client.get(url, headers=headers)
                if response.status_code == 200:
                    return response.content
        except Exception as e:
            print(f"Error downloading {url}: {e}")
        return None

    def _safe_filename(self, title: str) -> str:
        """Convert title to safe filename."""
        # Remove or replace unsafe characters
        safe = re.sub(r'[<>:"/\\|?*]', "", title)
        safe = safe.strip()
        # Limit length
        if len(safe) > 100:
            safe = safe[:100]
        return safe or "article"
